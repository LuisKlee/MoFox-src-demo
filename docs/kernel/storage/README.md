# Storage å­˜å‚¨æ¨¡å—æ–‡æ¡£

## æ¦‚è§ˆ

Storage æ¨¡å—æä¾›ç»Ÿä¸€çš„JSONæœ¬åœ°æŒä¹…åŒ–æ“ä½œï¼Œæ˜¯ MoFox kernel å±‚çš„æ ¸å¿ƒå­˜å‚¨ç»„ä»¶ã€‚è¯¥æ¨¡å—æä¾›äº†å®‰å…¨ã€é«˜æ•ˆã€æ˜“ç”¨çš„JSONæ–‡ä»¶å­˜å‚¨æ–¹æ¡ˆï¼Œæ”¯æŒCRUDæ“ä½œã€åŸå­å†™å…¥ã€è‡ªåŠ¨å¤‡ä»½ã€æ•°æ®å‹ç¼©ç­‰ä¼ä¸šçº§ç‰¹æ€§ã€‚

### è®¾è®¡ç†å¿µ

- **åŸå­æ€§**: ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶+é‡å‘½åæœºåˆ¶ä¿è¯å†™å…¥åŸå­æ€§
- **å®‰å…¨æ€§**: çº¿ç¨‹å®‰å…¨çš„å¹¶å‘æ“ä½œã€è‡ªåŠ¨å¤‡ä»½ã€æ•°æ®éªŒè¯
- **æ˜“ç”¨æ€§**: é¢å‘å¯¹è±¡çš„APIè®¾è®¡ã€ç±»å‹ç‰¹åŒ–ã€é“¾å¼æ“ä½œ
- **å¯é æ€§**: å¼‚å¸¸å¤„ç†å®Œå–„ã€å¤‡ä»½æ¢å¤æœºåˆ¶ã€å‹ç¼©å½’æ¡£æ”¯æŒ

### æ ¸å¿ƒç‰¹æ€§

âœ¨ **åŸå­å†™å…¥**: å…ˆå†™ä¸´æ—¶æ–‡ä»¶å†é‡å‘½åï¼Œé¿å…æ•°æ®æŸå  
ğŸ”’ **çº¿ç¨‹å®‰å…¨**: å†…ç½®é”æœºåˆ¶ï¼Œæ”¯æŒå¹¶å‘è¯»å†™  
ğŸ’¾ **è‡ªåŠ¨å¤‡ä»½**: å†™å…¥å‰è‡ªåŠ¨å¤‡ä»½ï¼Œæœ€å¤šä¿ç•™Nä¸ªå†å²ç‰ˆæœ¬  
ğŸ—œï¸ **æ•°æ®å‹ç¼©**: æ”¯æŒgzipå‹ç¼©ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´  
âœ… **æ•°æ®éªŒè¯**: å¯è‡ªå®šä¹‰éªŒè¯å‡½æ•°ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§  
ğŸ“¦ **ç±»å‹ç‰¹åŒ–**: æä¾›å­—å…¸ã€åˆ—è¡¨ã€æ—¥å¿—ä¸“ç”¨å­˜å‚¨å™¨  
ğŸ”„ **åŸå­æ›´æ–°**: è¯»å–-ä¿®æ”¹-å†™å…¥çš„åŸå­æ“ä½œ  

---

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
from kernel.storage import JSONStore

# åˆ›å»ºå­˜å‚¨å™¨
store = JSONStore('data.json', auto_create=True, auto_backup=True)

# å†™å…¥æ•°æ®
store.write({'name': 'MoFox', 'version': '1.0'})

# è¯»å–æ•°æ®
data = store.read()
print(data)  # {'name': 'MoFox', 'version': '1.0'}

# æ›´æ–°æ•°æ®
store.update(lambda data: {**data, 'updated': True})

# æ£€æŸ¥æ–‡ä»¶ä¿¡æ¯
print(f"æ–‡ä»¶å¤§å°: {store.get_size()} bytes")
print(f"æ–‡ä»¶å­˜åœ¨: {store.exists()}")
```

### å­—å…¸å­˜å‚¨å™¨

```python
from kernel.storage import DictJSONStore

# åˆ›å»ºå­—å…¸å­˜å‚¨å™¨
config = DictJSONStore('config.json')

# é”®å€¼æ“ä½œ
config.set('database', 'postgresql')
config.set('port', 5432)
config.set('debug', True)

# è¯»å–å€¼
db = config.get('database')  # 'postgresql'
timeout = config.get('timeout', 30)  # ä½¿ç”¨é»˜è®¤å€¼

# æ£€æŸ¥é”®
if config.has_key('debug'):
    print("Debug mode enabled")

# éå†
for key, value in config.items():
    print(f"{key}: {value}")

# åˆå¹¶é…ç½®
config.merge({'host': 'localhost', 'port': 3306}, overwrite=False)

# åˆ é™¤é”®
config.delete_key('debug')

# æ¸…ç©º
config.clear()
```

### åˆ—è¡¨å­˜å‚¨å™¨

```python
from kernel.storage import ListJSONStore

# åˆ›å»ºåˆ—è¡¨å­˜å‚¨å™¨
tasks = ListJSONStore('tasks.json')

# æ·»åŠ é¡¹ç›®
tasks.append({'id': 1, 'title': 'å­¦ä¹ Python', 'done': False})
tasks.append({'id': 2, 'title': 'å†™ä»£ç ', 'done': True})

# æ‰¹é‡æ·»åŠ 
tasks.extend([
    {'id': 3, 'title': 'æµ‹è¯•', 'done': False},
    {'id': 4, 'title': 'éƒ¨ç½²', 'done': False}
])

# è·å–é¡¹ç›®
first_task = tasks.get_at(0)
print(f"åˆ—è¡¨é•¿åº¦: {tasks.length()}")

# è¿‡æ»¤æœªå®Œæˆä»»åŠ¡
tasks.filter(lambda task: not task['done'])

# ç§»é™¤é¡¹ç›®
tasks.remove_at(0)

# æ¸…ç©ºåˆ—è¡¨
tasks.clear()
```

### æ—¥å¿—å­˜å‚¨å™¨

```python
from kernel.storage import LogStore
from datetime import datetime, timedelta

# åˆ›å»ºæ—¥å¿—å­˜å‚¨å™¨
logger = LogStore(
    directory='logs',
    prefix='app',
    max_entries_per_file=1000,
    auto_rotate=True
)

# æ·»åŠ æ—¥å¿—
logger.add_log({
    'level': 'INFO',
    'message': 'åº”ç”¨å¯åŠ¨',
    'user': 'admin'
})

logger.add_log({
    'level': 'ERROR',
    'message': 'è¿æ¥å¤±è´¥',
    'error': 'Connection timeout'
})

# æŸ¥è¯¢æ—¥å¿—
# è·å–æœ€è¿‘7å¤©çš„æ—¥å¿—
start = datetime.now() - timedelta(days=7)
logs = logger.get_logs(start_date=start)

# ä½¿ç”¨è¿‡æ»¤å™¨
error_logs = logger.get_logs(
    filter_func=lambda log: log.get('level') == 'ERROR'
)

# æ¸…ç†30å¤©å‰çš„æ—¥å¿—
deleted = logger.clear_old_logs(days=30)
print(f"åˆ é™¤äº† {deleted} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶")
```

---

## æ¶æ„è®¾è®¡

### ç±»å±‚æ¬¡ç»“æ„

```
JSONStore (åŸºç¡€å­˜å‚¨å™¨)
â”œâ”€â”€ DictJSONStore (å­—å…¸å­˜å‚¨å™¨)
â”œâ”€â”€ ListJSONStore (åˆ—è¡¨å­˜å‚¨å™¨)
â””â”€â”€ LogStore (æ—¥å¿—å­˜å‚¨å™¨)
```

### æ ¸å¿ƒç»„ä»¶

#### 1. JSONStore - åŸºç¡€å­˜å‚¨å™¨

é€šç”¨JSONæ–‡ä»¶å­˜å‚¨å™¨ï¼Œæä¾›åº•å±‚çš„è¯»å†™ã€å¤‡ä»½ã€å‹ç¼©ç­‰åŠŸèƒ½ã€‚

**ä¸»è¦æ–¹æ³•**:
- `read(default)` - è¯»å–æ•°æ®
- `write(data)` - å†™å…¥æ•°æ®ï¼ˆåŸå­æ“ä½œï¼‰
- `update(update_func)` - åŸå­æ›´æ–°
- `delete()` - åˆ é™¤æ–‡ä»¶
- `compress()` - å‹ç¼©æ–‡ä»¶
- `decompress()` - è§£å‹æ–‡ä»¶

#### 2. DictJSONStore - å­—å…¸å­˜å‚¨å™¨

ä¸“é—¨å¤„ç†å­—å…¸ç±»å‹æ•°æ®ï¼Œæä¾›é”®å€¼å¯¹æ“ä½œæ¥å£ã€‚

**ä¸»è¦æ–¹æ³•**:
- `get(key, default)` - è·å–å€¼
- `set(key, value)` - è®¾ç½®å€¼
- `delete_key(key)` - åˆ é™¤é”®
- `has_key(key)` - æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
- `keys()` / `values()` / `items()` - éå†æ“ä½œ
- `merge(other)` - åˆå¹¶å­—å…¸
- `clear()` - æ¸…ç©ºæ•°æ®

#### 3. ListJSONStore - åˆ—è¡¨å­˜å‚¨å™¨

ä¸“é—¨å¤„ç†åˆ—è¡¨ç±»å‹æ•°æ®ï¼Œæä¾›åˆ—è¡¨æ“ä½œæ¥å£ã€‚

**ä¸»è¦æ–¹æ³•**:
- `append(item)` - è¿½åŠ é¡¹ç›®
- `extend(items)` - æ‰©å±•åˆ—è¡¨
- `remove(item)` - ç§»é™¤é¡¹ç›®
- `remove_at(index)` - æŒ‰ç´¢å¼•ç§»é™¤
- `get_at(index)` - æŒ‰ç´¢å¼•è·å–
- `length()` - è·å–é•¿åº¦
- `filter(filter_func)` - è¿‡æ»¤é¡¹ç›®
- `clear()` - æ¸…ç©ºåˆ—è¡¨

#### 4. LogStore - æ—¥å¿—å­˜å‚¨å™¨

ä¸“é—¨ç”¨äºå­˜å‚¨æ—¥å¿—è®°å½•ï¼Œæ”¯æŒè‡ªåŠ¨è½®è½¬ã€æ—¶é—´èŒƒå›´æŸ¥è¯¢ã€‚

**ä¸»è¦æ–¹æ³•**:
- `add_log(log_entry)` - æ·»åŠ æ—¥å¿—
- `get_logs(start_date, end_date, filter_func)` - æŸ¥è¯¢æ—¥å¿—
- `clear_old_logs(days)` - æ¸…ç†æ—§æ—¥å¿—

---

## é«˜çº§ç‰¹æ€§

### 1. åŸå­å†™å…¥æœºåˆ¶

Storageæ¨¡å—ä½¿ç”¨"ä¸´æ—¶æ–‡ä»¶+åŸå­é‡å‘½å"æœºåˆ¶ç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼š

```python
def _write_data(self, data: Any) -> None:
    # 1. å†™å…¥ä¸´æ—¶æ–‡ä»¶
    temp_file = self.file_path.with_suffix('.tmp')
    with open(temp_file, 'w', encoding=self.encoding) as f:
        json.dump(data, f, indent=self.indent)
    
    # 2. åŸå­é‡å‘½åï¼ˆæ“ä½œç³»ç»Ÿçº§åˆ«ä¿è¯ï¼‰
    temp_file.replace(self.file_path)
```

**ä¼˜åŠ¿**:
- é¿å…å†™å…¥è¿‡ç¨‹ä¸­ç¨‹åºå´©æºƒå¯¼è‡´æ•°æ®æŸå
- æ“ä½œç³»ç»Ÿçº§åˆ«çš„åŸå­æ€§ä¿è¯
- è¯»å†™æ“ä½œäº’ä¸å½±å“

### 2. è‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿ

æ¯æ¬¡å†™å…¥å‰è‡ªåŠ¨åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ï¼š

```python
store = JSONStore(
    'config.json',
    auto_backup=True,      # å¯ç”¨è‡ªåŠ¨å¤‡ä»½
    max_backups=5          # ä¿ç•™æœ€è¿‘5ä¸ªå¤‡ä»½
)

store.write(data)
# è‡ªåŠ¨åˆ›å»º: config_backup_20260106_143022.json
```

**å¤‡ä»½æ–‡ä»¶å‘½åè§„åˆ™**: `{åŸæ–‡ä»¶å}_backup_{æ—¶é—´æˆ³}.{æ‰©å±•å}`

### 3. æ•°æ®éªŒè¯

å¯è‡ªå®šä¹‰éªŒè¯å‡½æ•°ç¡®ä¿æ•°æ®æœ‰æ•ˆæ€§ï¼š

```python
def validate_config(data):
    """éªŒè¯é…ç½®æ•°æ®"""
    if not isinstance(data, dict):
        return False
    required_keys = ['host', 'port', 'database']
    return all(key in data for key in required_keys)

store = JSONStore(
    'config.json',
    validate_func=validate_config
)

try:
    store.write({'host': 'localhost'})  # éªŒè¯å¤±è´¥
except ValidationError as e:
    print(f"éªŒè¯å¤±è´¥: {e}")
```

### 4. çº¿ç¨‹å®‰å…¨

æ‰€æœ‰æ“ä½œéƒ½ä½¿ç”¨ `threading.Lock` ä¿æŠ¤ï¼š

```python
import threading

store = DictJSONStore('shared.json')

def worker(worker_id):
    for i in range(100):
        store.set(f'worker_{worker_id}_{i}', {'value': i})

# å¤šçº¿ç¨‹å¹¶å‘å†™å…¥
threads = [threading.Thread(target=worker, args=(i,)) for i in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()

# æ•°æ®å®Œæ•´æ€§å¾—åˆ°ä¿è¯
```

### 5. åŸå­æ›´æ–°æ“ä½œ

è¯»å–-ä¿®æ”¹-å†™å…¥è¿‡ç¨‹æ˜¯åŸå­çš„ï¼š

```python
store = DictJSONStore('counter.json')

# çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨é€’å¢
def increment_counter():
    store.update(lambda data: {
        **data,
        'count': data.get('count', 0) + 1
    })

# å¤šçº¿ç¨‹ç¯å¢ƒä¸‹è®¡æ•°å‡†ç¡®
```

### 6. æ•°æ®å‹ç¼©

æ”¯æŒgzipå‹ç¼©ä»¥èŠ‚çœç©ºé—´ï¼š

```python
store = JSONStore('large_data.json')

# å‹ç¼©åˆ°é»˜è®¤ä½ç½® (large_data.json.gz)
compressed_path = store.compress()

# å‹ç¼©åˆ°æŒ‡å®šä½ç½®
compressed_path = store.compress('backup/data.gz')

# è§£å‹
store.decompress('backup/data.gz')
```

---

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: åº”ç”¨é…ç½®ç®¡ç†

```python
from kernel.storage import DictJSONStore

class ConfigManager:
    def __init__(self, config_file='config.json'):
        self.store = DictJSONStore(config_file, auto_backup=True)
    
    def get_database_config(self):
        return {
            'host': self.store.get('db_host', 'localhost'),
            'port': self.store.get('db_port', 5432),
            'database': self.store.get('db_name', 'myapp'),
            'user': self.store.get('db_user', 'admin'),
        }
    
    def update_setting(self, key, value):
        self.store.set(key, value)
    
    def reset_to_defaults(self):
        defaults = {
            'db_host': 'localhost',
            'db_port': 5432,
            'theme': 'dark',
            'language': 'zh-CN'
        }
        self.store.write(defaults)

# ä½¿ç”¨
config = ConfigManager()
config.update_setting('theme', 'light')
db_config = config.get_database_config()
```

### åœºæ™¯2: ä»»åŠ¡é˜Ÿåˆ—

```python
from kernel.storage import ListJSONStore
from datetime import datetime

class TaskQueue:
    def __init__(self, queue_file='tasks.json'):
        self.store = ListJSONStore(queue_file)
    
    def add_task(self, task_type, data):
        task = {
            'id': self._generate_id(),
            'type': task_type,
            'data': data,
            'status': 'pending',
            'created_at': datetime.now().isoformat()
        }
        self.store.append(task)
    
    def get_pending_tasks(self):
        tasks = self.store.read(default=[])
        return [t for t in tasks if t.get('status') == 'pending']
    
    def mark_completed(self, task_id):
        def update(tasks):
            for task in tasks:
                if task.get('id') == task_id:
                    task['status'] = 'completed'
                    task['completed_at'] = datetime.now().isoformat()
            return tasks
        
        self.store.update(update)
    
    def _generate_id(self):
        import uuid
        return str(uuid.uuid4())

# ä½¿ç”¨
queue = TaskQueue()
queue.add_task('email', {'to': 'user@example.com', 'subject': 'Hello'})
pending = queue.get_pending_tasks()
```

### åœºæ™¯3: ç”¨æˆ·æ•°æ®å­˜å‚¨

```python
from kernel.storage import DictJSONStore

class UserStore:
    def __init__(self, data_dir='users'):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
    
    def save_user(self, user_id, user_data):
        store = DictJSONStore(self.data_dir / f"user_{user_id}.json")
        store.write(user_data)
    
    def get_user(self, user_id):
        store = DictJSONStore(self.data_dir / f"user_{user_id}.json")
        return store.read(default=None)
    
    def update_user(self, user_id, updates):
        store = DictJSONStore(self.data_dir / f"user_{user_id}.json")
        store.merge(updates, overwrite=True)
    
    def delete_user(self, user_id):
        store = DictJSONStore(self.data_dir / f"user_{user_id}.json")
        store.delete(create_backup=True)

# ä½¿ç”¨
users = UserStore()
users.save_user('001', {'name': 'Alice', 'email': 'alice@example.com'})
users.update_user('001', {'last_login': datetime.now().isoformat()})
user = users.get_user('001')
```

### åœºæ™¯4: æ“ä½œæ—¥å¿—å®¡è®¡

```python
from kernel.storage import LogStore

class AuditLogger:
    def __init__(self, log_dir='audit_logs'):
        self.log_store = LogStore(
            directory=log_dir,
            prefix='audit',
            max_entries_per_file=5000,
            auto_rotate=True
        )
    
    def log_action(self, user_id, action, resource, details=None):
        self.log_store.add_log({
            'user_id': user_id,
            'action': action,
            'resource': resource,
            'details': details,
            'ip_address': self._get_ip(),
        })
    
    def get_user_actions(self, user_id, days=7):
        start = datetime.now() - timedelta(days=days)
        return self.log_store.get_logs(
            start_date=start,
            filter_func=lambda log: log.get('user_id') == user_id
        )
    
    def get_security_events(self):
        critical_actions = ['login_failed', 'permission_denied', 'data_deleted']
        return self.log_store.get_logs(
            filter_func=lambda log: log.get('action') in critical_actions
        )
    
    def _get_ip(self):
        # å®é™…å®ç°ä¸­è·å–çœŸå®IP
        return '127.0.0.1'

# ä½¿ç”¨
audit = AuditLogger()
audit.log_action('user_001', 'login', '/api/auth')
audit.log_action('user_001', 'update', '/api/users/profile')
actions = audit.get_user_actions('user_001', days=7)
```

---

## å¼‚å¸¸å¤„ç†

### å¼‚å¸¸ç±»å‹

```python
from kernel.storage import JSONStoreError, FileNotFoundError, ValidationError

# JSONStoreError - åŸºç¡€å¼‚å¸¸ç±»
# FileNotFoundError - æ–‡ä»¶ä¸å­˜åœ¨
# ValidationError - æ•°æ®éªŒè¯å¤±è´¥
```

### æ¨èçš„å¼‚å¸¸å¤„ç†æ¨¡å¼

```python
from kernel.storage import DictJSONStore, JSONStoreError, ValidationError

store = DictJSONStore('config.json')

try:
    # è¯»å–æ“ä½œ
    data = store.read()
except FileNotFoundError:
    print("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    data = store.read(default={})
except JSONStoreError as e:
    print(f"è¯»å–å¤±è´¥: {e}")
    data = {}

try:
    # å†™å…¥æ“ä½œ
    store.write(data)
except ValidationError as e:
    print(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
except JSONStoreError as e:
    print(f"å†™å…¥å¤±è´¥: {e}")
```

---

## æ€§èƒ½å»ºè®®

### 1. é€‰æ‹©åˆé€‚çš„å­˜å‚¨å™¨ç±»å‹

```python
# âŒ ä¸æ¨èï¼šç”¨é€šç”¨å­˜å‚¨å™¨å¤„ç†å­—å…¸
store = JSONStore('config.json')
data = store.read()
data['key'] = 'value'
store.write(data)

# âœ… æ¨èï¼šä½¿ç”¨å­—å…¸å­˜å‚¨å™¨
store = DictJSONStore('config.json')
store.set('key', 'value')
```

### 2. æ‰¹é‡æ“ä½œ

```python
# âŒ ä¸æ¨èï¼šå¤šæ¬¡å†™å…¥
for i in range(100):
    store.append(i)  # 100æ¬¡æ–‡ä»¶æ“ä½œ

# âœ… æ¨èï¼šæ‰¹é‡å†™å…¥
items = list(range(100))
store.extend(items)  # 1æ¬¡æ–‡ä»¶æ“ä½œ
```

### 3. åˆç†ä½¿ç”¨å¤‡ä»½

```python
# é¢‘ç¹å†™å…¥åœºæ™¯ï¼Œè€ƒè™‘å…³é—­è‡ªåŠ¨å¤‡ä»½
store = JSONStore('cache.json', auto_backup=False)

# é‡è¦æ•°æ®åœºæ™¯ï¼Œå¯ç”¨è‡ªåŠ¨å¤‡ä»½
store = JSONStore('user_data.json', auto_backup=True, max_backups=10)
```

### 4. å¤§æ–‡ä»¶å¤„ç†

```python
# å¯¹äºå¤§å‹æ—¥å¿—æ–‡ä»¶ï¼Œä½¿ç”¨LogStoreè‡ªåŠ¨è½®è½¬
logger = LogStore(
    directory='logs',
    max_entries_per_file=1000,  # é™åˆ¶å•æ–‡ä»¶å¤§å°
    auto_rotate=True
)

# å®šæœŸæ¸…ç†æ—§æ–‡ä»¶
logger.clear_old_logs(days=30)
```

### 5. é¿å…é¢‘ç¹çš„å®Œæ•´è¯»å–

```python
# âŒ ä¸æ¨èï¼šæ¯æ¬¡éƒ½è¯»å–å®Œæ•´æ•°æ®
def get_value(key):
    data = store.read()
    return data.get(key)

# âœ… æ¨èï¼šä½¿ç”¨å­—å…¸å­˜å‚¨å™¨çš„ç›´æ¥è®¿é—®
def get_value(key):
    return store.get(key)  # å†…éƒ¨ä¼˜åŒ–äº†è¯»å–
```

---

## ä¸å…¶ä»–æ¨¡å—é›†æˆ

### ä¸Loggeræ¨¡å—é›†æˆ

```python
from kernel.logger import setup_logger
from kernel.storage import DictJSONStore

logger = setup_logger('storage_demo')

store = DictJSONStore('config.json')

try:
    data = store.read()
    logger.info(f"æˆåŠŸè¯»å–é…ç½®: {len(data)} é¡¹")
except Exception as e:
    logger.error(f"è¯»å–é…ç½®å¤±è´¥: {e}", exc_info=True)
```

### ä¸Configæ¨¡å—é›†æˆ

```python
from kernel.config import Config
from kernel.storage import DictJSONStore

class PersistentConfig(Config):
    def __init__(self, config_file='config.json'):
        super().__init__()
        self.store = DictJSONStore(config_file)
        self._load_from_store()
    
    def _load_from_store(self):
        data = self.store.read(default={})
        for key, value in data.items():
            self.set(key, value)
    
    def save(self):
        data = self.to_dict()
        self.store.write(data)
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†å¹¶å‘å†™å…¥ï¼Ÿ

A: Storageæ¨¡å—å·²å†…ç½®çº¿ç¨‹å®‰å…¨æœºåˆ¶ï¼Œå¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯ç›´æ¥ä½¿ç”¨ï¼š

```python
import threading

store = DictJSONStore('shared.json')

def worker(worker_id):
    store.set(f'worker_{worker_id}', {'data': 'value'})

threads = [threading.Thread(target=worker, args=(i,)) for i in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()
```

### Q2: å¦‚ä½•æ¢å¤å¤‡ä»½æ•°æ®ï¼Ÿ

A: å¤‡ä»½æ–‡ä»¶æ˜¯æ ‡å‡†JSONæ ¼å¼ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼š

```python
# æ–¹æ³•1: æ‰‹åŠ¨è¯»å–å¤‡ä»½æ–‡ä»¶
backup_store = JSONStore('config_backup_20260106_143022.json')
data = backup_store.read()
store.write(data)

# æ–¹æ³•2: ç›´æ¥é‡å‘½åå¤‡ä»½æ–‡ä»¶
import shutil
shutil.copy('config_backup_20260106_143022.json', 'config.json')
```

### Q3: å¦‚ä½•å¤„ç†å¤§å‹JSONæ–‡ä»¶ï¼Ÿ

A: è€ƒè™‘ä»¥ä¸‹ç­–ç•¥ï¼š

```python
# 1. ä½¿ç”¨LogStoreè‡ªåŠ¨åˆ†ç‰‡
logger = LogStore(directory='logs', max_entries_per_file=1000)

# 2. åˆ†ç¦»å­˜å‚¨
user_store = DictJSONStore('users/user_{id}.json')  # æ¯ä¸ªç”¨æˆ·å•ç‹¬æ–‡ä»¶

# 3. å®šæœŸæ¸…ç†å’Œå‹ç¼©
store.compress()
logger.clear_old_logs(days=30)
```

### Q4: å¦‚ä½•åœ¨æµ‹è¯•ä¸­ä½¿ç”¨ï¼Ÿ

A: ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æˆ–å†…å­˜è·¯å¾„ï¼š

```python
import tempfile
from pathlib import Path

def test_storage():
    with tempfile.TemporaryDirectory() as tmpdir:
        store = DictJSONStore(Path(tmpdir) / 'test.json')
        store.set('key', 'value')
        assert store.get('key') == 'value'
```

---

## æ›´å¤šæ–‡æ¡£

- [é…ç½®æŒ‡å—](./CONFIGURATION_GUIDE.md) - è¯¦ç»†çš„é…ç½®å‚æ•°è¯´æ˜
- [APIå‚è€ƒ](./API_REFERENCE.md) - å®Œæ•´çš„APIæ–‡æ¡£
- [æœ€ä½³å®è·µ](./BEST_PRACTICES.md) - ä½¿ç”¨æ¨¡å¼å’Œå»ºè®®
- [æ•…éšœæ’æŸ¥](./TROUBLESHOOTING.md) - å¸¸è§é—®é¢˜è§£å†³

---

## ç‰ˆæœ¬å†å²

- **v1.0.0** (2026-01-06)
  - åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
  - æä¾›JSONStoreã€DictJSONStoreã€ListJSONStoreã€LogStoreå››ç§å­˜å‚¨å™¨
  - æ”¯æŒåŸå­å†™å…¥ã€è‡ªåŠ¨å¤‡ä»½ã€æ•°æ®å‹ç¼©ã€çº¿ç¨‹å®‰å…¨ç­‰æ ¸å¿ƒç‰¹æ€§
