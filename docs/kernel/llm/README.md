# LLM æ¨¡å—æ–‡æ¡£

MoFox LLM æ¨¡å—æä¾›ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹äº¤äº’æ¥å£ï¼Œæ”¯æŒå¤šä¸ªä¸»æµ LLM æä¾›å•†ã€‚

## ğŸ“š æ–‡æ¡£å¯¼èˆª

- [API å‚è€ƒ](API_REFERENCE.md) - å®Œæ•´çš„ API æ–‡æ¡£
- [æœ€ä½³å®è·µ](BEST_PRACTICES.md) - ä½¿ç”¨å»ºè®®å’Œæœ€ä½³å®è·µ
- [å¿«é€Ÿå‚è€ƒ](QUICK_REFERENCE.md) - å¸¸ç”¨åŠŸèƒ½é€ŸæŸ¥
- [æç¤ºè¯æŒ‡å—](PROMPT_GUIDE.md) - æç¤ºè¯å·¥ç¨‹æœ€ä½³å®è·µ
- [å·¥å…·è°ƒç”¨æŒ‡å—](TOOL_CALLING_GUIDE.md) - Function Calling å®Œæ•´æŒ‡å—

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### å¤šæä¾›å•†æ”¯æŒ
- **OpenAI** - GPT-3.5/GPT-4 ç³»åˆ—
- **Google Gemini** - Gemini Pro/Ultra
- **AWS Bedrock** - Claudeã€Llamaã€Titan ç­‰

### ç»Ÿä¸€æ¥å£
```python
# æ‰€æœ‰æä¾›å•†ä½¿ç”¨ç›¸åŒçš„ API
response = await generate(
    model="gpt-4",
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    provider="openai"
)
```

### åŠŸèƒ½å®Œæ•´
- âœ… æ–‡æœ¬ç”Ÿæˆ
- âœ… æµå¼å“åº”
- âœ… å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰
- âœ… å¤šæ¨¡æ€ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰
- âœ… æ–‡æœ¬åµŒå…¥
- âœ… å¼‚æ­¥æ”¯æŒ
- âœ… è‡ªåŠ¨é‡è¯•
- âœ… æ—¥å¿—é›†æˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install openai aiohttp boto3

# å¯é€‰ä¾èµ–
pip install pillow  # å›¾åƒå¤„ç†
```

> Windows æç¤ºï¼šå¦‚é‡åˆ° `pip` è¯»å– `requirements.txt` çš„ç¼–ç é”™è¯¯ï¼ˆä¾‹å¦‚ `gbk` è§£ç å¤±è´¥ï¼‰ï¼Œå¯å…ˆåœ¨ç»ˆç«¯æ‰§è¡Œ `chcp 65001` åˆ‡æ¢åˆ° UTF-8ï¼Œå†å®‰è£…ä¾èµ–ï¼›æˆ–å…ˆå•ç‹¬å®‰è£…å…³é”®ä¾èµ–éªŒè¯ï¼š

```bash
chcp 65001
py -3.11 -m pip install -r requirements.txt

# ä»…å®‰è£…å…³é”®ä¾èµ–éªŒè¯è¿è¡Œ
py -3.11 -m pip install "openai>=1.10.0"
py -3.11 -m pytest -q
```

> VS Code è§£é‡Šå™¨ï¼šè‹¥ç¼–è¾‘å™¨æŠ¥ â€œæ— æ³•è§£æå¯¼å…¥ openaiâ€ï¼Œè¯·åœ¨ VS Code å³ä¸‹è§’é€‰æ‹©ä¸ä½ è¿è¡Œä¸€è‡´çš„ Python è§£é‡Šå™¨ï¼ˆæ¨è 3.11ï¼‰ï¼Œæˆ–åœ¨å·¥ä½œåŒºè®¾ç½®ä¸­é…ç½® `python.defaultInterpreterPath`ã€‚

### åŸºç¡€ä½¿ç”¨

```python
from kernel.llm import generate, MessageBuilder

# åˆ›å»ºæ¶ˆæ¯
messages = [
    MessageBuilder.create_system_message("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"),
    MessageBuilder.create_user_message("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
]

# ç”Ÿæˆå“åº”
response = await generate(
    model="gpt-4",
    messages=messages,
    provider="openai",
    temperature=0.7
)

print(response.content)
print(f"ä½¿ç”¨ tokens: {response.usage['total_tokens']}")
```

### æµå¼ç”Ÿæˆ

```python
from kernel.llm import stream_generate

async for chunk in stream_generate(
    model="gpt-4",
    messages=messages,
    provider="openai"
):
    print(chunk.content, end="", flush=True)
```

### å·¥å…·è°ƒç”¨

```python
from kernel.llm import generate_with_tools, ToolBuilder

# å®šä¹‰å·¥å…·
tool = ToolBuilder.create_tool(
    name="get_weather",
    description="è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
    parameters=[
        ToolBuilder.create_parameter(
            name="city",
            param_type="string",
            description="åŸå¸‚åç§°",
            required=True
        ),
        ToolBuilder.create_parameter(
            name="unit",
            param_type="string",
            description="æ¸©åº¦å•ä½",
            enum=["celsius", "fahrenheit"],
            default="celsius"
        )
    ]
)

# ä½¿ç”¨å·¥å…·
response = await generate_with_tools(
    model="gpt-4",
    messages=[MessageBuilder.create_user_message("åŒ—äº¬ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ")],
    tools=[tool],
    provider="openai"
)

# æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
if response.tool_calls:
    for call in response.tool_calls:
        print(f"è°ƒç”¨å·¥å…·: {call['function']['name']}")
        print(f"å‚æ•°: {call['function']['arguments']}")
```

### å¤šæ¨¡æ€è¾“å…¥

```python
from kernel.llm import MessageBuilder

# åˆ›å»ºåŒ…å«å›¾åƒçš„æ¶ˆæ¯
message = MessageBuilder.create_multimodal_message(
    text="è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
    image_paths=["image.jpg"]  # è‡ªåŠ¨å¤„ç†å›¾åƒ
)

response = await generate(
    model="gpt-4-vision-preview",
    messages=[message],
    provider="openai"
)
```

## ğŸ”§ å®¢æˆ·ç«¯æ³¨å†Œ

### æ³¨å†Œè‡ªå®šä¹‰å®¢æˆ·ç«¯

```python
from kernel.llm import register_client, OpenAIClient

# æ³¨å†Œ OpenAI å®¢æˆ·ç«¯
client = OpenAIClient(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1"
)

register_client("openai", client)
```

### ä½¿ç”¨è‡ªå®šä¹‰ API

```python
# æ³¨å†Œå…¼å®¹ OpenAI çš„ APIï¼ˆå¦‚ DeepSeekï¼‰
deepseek_client = OpenAIClient(
    api_key="your-deepseek-key",
    base_url="https://api.deepseek.com/v1"
)

register_client("deepseek", deepseek_client)

# ä½¿ç”¨
response = await generate(
    model="deepseek-chat",
    messages=messages,
    provider="deepseek"
)
```

## ğŸ“ æç¤ºè¯ç®¡ç†

### ä½¿ç”¨é¢„è®¾æç¤ºè¯

```python
from kernel.llm import get_system_prompt

# è·å–é¢„è®¾ç³»ç»Ÿæç¤ºè¯
system_prompt = get_system_prompt("coding")  # ç¼–ç¨‹åŠ©æ‰‹
# system_prompt = get_system_prompt("translation")  # ç¿»è¯‘åŠ©æ‰‹
# system_prompt = get_system_prompt("data_analysis")  # æ•°æ®åˆ†æåŠ©æ‰‹

messages = [
    MessageBuilder.create_system_message(system_prompt),
    MessageBuilder.create_user_message("ç”¨ Python å®ç°å¿«é€Ÿæ’åº")
]
```

### ä½¿ç”¨æç¤ºè¯æ¨¡æ¿

```python
from kernel.llm import PromptTemplates

# é—®ç­”æ¨¡æ¿
prompt = PromptTemplates.QA_TEMPLATE.substitute(
    context="Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...",
    question="Python çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"
)

# æ€»ç»“æ¨¡æ¿
prompt = PromptTemplates.SUMMARY_TEMPLATE.substitute(
    content="é•¿ç¯‡æ–‡ç« å†…å®¹...",
    max_length=200
)
```

### è‡ªå®šä¹‰æç¤ºè¯æ„å»º

```python
from kernel.llm import PromptBuilder

# æ„å»ºç»“æ„åŒ–æç¤ºè¯
prompt = PromptBuilder.build_system_prompt(
    role="ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆ",
    capabilities=[
        "åˆ†ææ•°æ®è¶‹åŠ¿",
        "ç”Ÿæˆå¯è§†åŒ–å»ºè®®",
        "è§£é‡Šç»Ÿè®¡ç»“æœ"
    ],
    constraints=[
        "ä½¿ç”¨å‡†ç¡®çš„ç»Ÿè®¡æœ¯è¯­",
        "æä¾›å¯æ“ä½œçš„å»ºè®®",
        "é¿å…è¿‡åº¦æŠ€æœ¯åŒ–"
    ],
    tone="ä¸“ä¸šè€Œå‹å¥½"
)
```

## ğŸ¨ å“åº”å¤„ç†

### è§£æå“åº”

```python
from kernel.llm import ResponseParser

# è§£æå®Œæ•´å“åº”
parsed = ResponseParser.parse_completion(raw_response)

# æå–å†…å®¹
content = ResponseParser.extract_content(raw_response)

# æå–å·¥å…·è°ƒç”¨
tool_calls = ResponseParser.extract_tool_calls(raw_response)

# æå–ä½¿ç”¨æƒ…å†µ
usage = ResponseParser.extract_usage(raw_response)
```

### å¤„ç†æµå¼å“åº”

```python
from kernel.llm import stream_generate

full_content = ""
async for chunk in stream_generate(model="gpt-4", messages=messages):
    full_content += chunk.content
    
    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
    if chunk.finish_reason:
        print(f"\nå®ŒæˆåŸå› : {chunk.finish_reason}")
        break
```

## ğŸ”„ é”™è¯¯å¤„ç†

```python
from kernel.llm import (
    generate,
    LLMError,
    AuthenticationError,
    RateLimitError,
    ContextLengthExceededError
)

try:
    response = await generate(
        model="gpt-4",
        messages=messages,
        provider="openai"
    )
except AuthenticationError as e:
    print(f"è®¤è¯å¤±è´¥: {e}")
except RateLimitError as e:
    print(f"é€Ÿç‡é™åˆ¶: {e}")
    # å®ç°é€€é¿é‡è¯•
except ContextLengthExceededError as e:
    print(f"ä¸Šä¸‹æ–‡è¿‡é•¿: {e}")
    # æˆªæ–­æ¶ˆæ¯æˆ–ä½¿ç”¨æ›´å¤§çª—å£çš„æ¨¡å‹
except LLMError as e:
    print(f"LLM é”™è¯¯: {e}")
```

## ğŸ“Š ä½¿ç”¨ç»Ÿè®¡

```python
# å“åº”åŒ…å«è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡
response = await generate(model="gpt-4", messages=messages)

print(f"æç¤ºè¯ tokens: {response.usage['prompt_tokens']}")
print(f"ç”Ÿæˆ tokens: {response.usage['completion_tokens']}")
print(f"æ€»è®¡ tokens: {response.usage['total_tokens']}")
print(f"å®ŒæˆåŸå› : {response.finish_reason}")
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°

```python
response = await generate(
    model="gpt-4",
    messages=messages,
    temperature=0.9,          # åˆ›é€ æ€§ (0-2)
    max_tokens=1000,          # æœ€å¤§è¾“å‡ºé•¿åº¦
    top_p=0.95,               # æ ¸é‡‡æ ·
    frequency_penalty=0.5,    # é¢‘ç‡æƒ©ç½š
    presence_penalty=0.5,     # å­˜åœ¨æƒ©ç½š
    stop=["###", "---"]       # åœæ­¢åºåˆ—
)
```

### JSON æ¨¡å¼

```python
# è¦æ±‚ JSON æ ¼å¼å“åº”
response = await generate(
    model="gpt-4-1106-preview",
    messages=[
        MessageBuilder.create_system_message(get_system_prompt("json")),
        MessageBuilder.create_user_message("åˆ†æè¿™ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ")
    ],
    response_format={"type": "json_object"}
)

import json
result = json.loads(response.content)
```

### æ‰¹é‡åµŒå…¥

```python
from kernel.llm import create_embeddings

texts = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
    "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†æ–‡æœ¬æ•°æ®"
]

embeddings = await create_embeddings(
    texts=texts,
    model="text-embedding-ada-002",
    provider="openai"
)

# embeddings æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå‘é‡
print(f"ç”Ÿæˆäº† {len(embeddings)} ä¸ªå‘é‡")
print(f"å‘é‡ç»´åº¦: {len(embeddings[0])}")
```

## ğŸ” è°ƒè¯•å’Œæ—¥å¿—

LLM æ¨¡å—é›†æˆäº† kernel.loggerï¼Œè‡ªåŠ¨è®°å½•å…³é”®æ“ä½œï¼š

```python
# æ—¥å¿—ä¼šè‡ªåŠ¨è®°å½•ï¼š
# - å®¢æˆ·ç«¯åˆå§‹åŒ–
# - API è°ƒç”¨ï¼ˆDEBUG çº§åˆ«ï¼‰
# - ä½¿ç”¨ç»Ÿè®¡ï¼ˆDEBUG çº§åˆ«ï¼‰
# - é”™è¯¯ä¿¡æ¯ï¼ˆERROR çº§åˆ«ï¼‰

# æŸ¥çœ‹æ—¥å¿—é…ç½®
from kernel.logger import get_logger

logger = get_logger("kernel.llm")
logger.setLevel("DEBUG")  # æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å®¢æˆ·ç«¯å¤ç”¨

```python
from kernel.llm import LLMRequestManager

# åˆ›å»ºç®¡ç†å™¨ï¼ˆä¼šç¼“å­˜å®¢æˆ·ç«¯ï¼‰
manager = LLMRequestManager()

# å¤šæ¬¡è°ƒç”¨ä¼šå¤ç”¨å®¢æˆ·ç«¯
for i in range(10):
    response = await manager.generate(
        LLMRequest(model="gpt-4", messages=messages)
    )

# æ¸…ç†
await manager.close()
```

### å¹¶å‘è¯·æ±‚

```python
import asyncio

# å¹¶å‘è°ƒç”¨å¤šä¸ªæ¨¡å‹
tasks = [
    generate(model="gpt-3.5-turbo", messages=messages, provider="openai"),
    generate(model="gemini-pro", messages=messages, provider="gemini"),
    generate(model="anthropic.claude-v2", messages=messages, provider="bedrock")
]

responses = await asyncio.gather(*tasks)
```

## ğŸ” å®‰å…¨æœ€ä½³å®è·µ

1. **API å¯†é’¥ç®¡ç†**
```python
import os

# ä»ç¯å¢ƒå˜é‡è¯»å–
api_key = os.getenv("OPENAI_API_KEY")

client = OpenAIClient(api_key=api_key)
```

2. **è¾“å…¥éªŒè¯**
```python
from kernel.llm import LLMRequest

request = LLMRequest(
    model="gpt-4",
    messages=messages,
    max_tokens=1000  # é™åˆ¶è¾“å‡ºé•¿åº¦
)

# éªŒè¯è¯·æ±‚
request.validate()  # ä¼šæŠ›å‡º ValidationError å¦‚æœæ— æ•ˆ
```

3. **å†…å®¹è¿‡æ»¤**
```python
# å®ç°è‡ªå®šä¹‰å†…å®¹è¿‡æ»¤
def filter_content(text: str) -> bool:
    # æ£€æŸ¥æ•æ„Ÿå†…å®¹
    return True

if filter_content(user_input):
    response = await generate(model="gpt-4", messages=messages)
```

## ğŸ“– æ›´å¤šèµ„æº

- [API å®Œæ•´å‚è€ƒ](API_REFERENCE.md)
- [æœ€ä½³å®è·µè¯¦è§£](BEST_PRACTICES.md)
- [æç¤ºè¯å·¥ç¨‹](PROMPT_GUIDE.md)
- [å·¥å…·è°ƒç”¨è¯¦è§£](TOOL_CALLING_GUIDE.md)
- [æ•…éšœæ’é™¤](TROUBLESHOOTING.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

## ğŸ“„ è®¸å¯

æœ¬æ¨¡å—éµå¾ª MoFox é¡¹ç›®çš„è®¸å¯åè®®ã€‚
